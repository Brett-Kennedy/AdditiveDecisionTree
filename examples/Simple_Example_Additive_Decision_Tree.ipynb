{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7283a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston, load_diabetes, load_linnerud \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "\n",
    "# Todo: remove once have pip install\n",
    "import sys  \n",
    "sys.path.insert(0, 'C:\\python_projects\\AdditiveDecisionTree_project\\AdditiveDecisionTree') \n",
    "from AdditiveDecisionTree import AdditiveDecisionTreeClasssifier, AdditiveDecisionTreeRegressor\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74761305",
   "metadata": {},
   "source": [
    "## Methods used to load the toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80859ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification datasets \n",
    "\n",
    "def get_iris():\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X = pd.DataFrame(X, columns=iris['feature_names'])\n",
    "    y = pd.Series(y)\n",
    "    return X, y\n",
    "\n",
    "def get_breast_cancer():\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    return X,y\n",
    "\n",
    "def get_wine():\n",
    "    X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "    return X,y\n",
    "\n",
    "# Regression datasets\n",
    "\n",
    "def get_boston():\n",
    "    data = load_boston()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    return X,y\n",
    "\n",
    "def get_diabetes():\n",
    "    data = load_diabetes()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    return X,y\n",
    "\n",
    "def get_linnerud():\n",
    "    data = load_linnerud(as_frame=True)\n",
    "    X = data.data\n",
    "    y = data.target['Weight']\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fac78",
   "metadata": {},
   "source": [
    "## Example using sklearn's Decision Tree and AddtiveDecisionTree on a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898c2ca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris\n",
      "Standard DT: Training score: 1.0, Testing score: 0.97, Complexity: 13\n",
      "Additive DT: Training score: 0.96, Testing score: 0.88, Complexity: 5\n",
      "\n",
      "Wine\n",
      "Standard DT: Training score: 1.0, Testing score: 0.92, Complexity: 13\n",
      "Additive DT: Training score: 0.97, Testing score: 0.95, Complexity: 7\n",
      "\n",
      "Breast Cancer\n",
      "Standard DT: Training score: 0.99, Testing score: 0.92, Complexity: 23\n",
      "Additive DT: Training score: 0.97, Testing score: 0.91, Complexity: 11\n"
     ]
    }
   ],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does not properly test its accuracy\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training score: {round(score_train,2)}, Testing score: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeClasssifier(max_depth=4, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "    \n",
    "X,y = get_iris()\n",
    "evaluate_dataset(\"Iris\", X,y)\n",
    "\n",
    "X,y = get_wine()\n",
    "evaluate_dataset(\"Wine\", X,y)\n",
    "\n",
    "X,y = get_breast_cancer()\n",
    "adt = evaluate_dataset(\"Breast Cancer\", X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e883f6",
   "metadata": {},
   "source": [
    "## Summary Output of the AdditiveDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575195ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************\n",
      "Generated Tree\n",
      "********************************************************\n",
      "# Nodes: 9\n",
      "Left Chidren: [1, 3, 5, -2, -2, 7, -2, -2, -2]\n",
      "Right Chidren: [2, 4, 6, -2, -2, 8, -2, -2, -2]\n",
      "# Rows:  [426, 260, 166, 252, 8, 30, 136, 14, 16]\n",
      "Features: [7, 20, 23, -100, -2, 21, -2, -2, -2]\n",
      "Features in additive nodes: [[], [], [], [1, 13], [], [], [], [], []]\n",
      "Thresholds: [0.04891999997198582, 17.589999198913574, 785.7999877929688, 21.574999809265137, -2, 23.739999771118164, -2, -2, -2]\n",
      "Depths: [0, 1, 1, 2, 2, 2, 2, 3, 3]\n",
      "Can split:  [True, True, True, True, True, True, True, True, True]\n",
      "Class counts: [[159, 267], [13, 247], [146, 20], [7, 245], [6, 2], [13, 17], [133, 3], [0, 14], [13, 3]]\n",
      "Leaf Class Counts: [[7, 245], [6, 2], [133, 3], [0, 14], [13, 3]]\n",
      "Node igr:  [0.4156254639152989, 0.2031712696855239, 0.29661687709662865, 0.18239393289682015, -2, 0.43241893359216155, -2, -2, -2]\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9add9",
   "metadata": {},
   "source": [
    "## Explanations of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369d85db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial distribution of classes: [0, 1]: [159, 267]\n",
      "\n",
      "\n",
      "Prediction for row 0: 0 -- Correct\n",
      "Path: [0, 2, 6]\n",
      "mean concave points is greater than 0.04891999997198582 (has value: 0.1471) --> (Class distribution: [146, 20]\n",
      "AND worst area is greater than 785.7999877929688 (has value: 2019.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "Prediction for row 1: 0 -- Correct\n",
      "Path: [0, 2, 6]\n",
      "mean concave points is greater than 0.04891999997198582 (has value: 0.07017) --> (Class distribution: [146, 20]\n",
      "AND worst area is greater than 785.7999877929688 (has value: 1956.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "Prediction for row 2: 0 -- Correct\n",
      "Path: [0, 2, 6]\n",
      "mean concave points is greater than 0.04891999997198582 (has value: 0.1279) --> (Class distribution: [146, 20]\n",
      "AND worst area is greater than 785.7999877929688 (has value: 1709.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "Prediction for row 3: 0 -- Correct\n",
      "Path: [0, 2, 5, 8]\n",
      "mean concave points is greater than 0.04891999997198582 (has value: 0.1052) --> (Class distribution: [146, 20]\n",
      "AND worst area is less than 785.7999877929688 (has value: 567.7) --> (Class distribution: [13, 17]\n",
      "AND worst texture is greater than 23.739999771118164 (has value: 26.5) --> (Class distribution: [13, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "Prediction for row 4: 0 -- Correct\n",
      "Path: [0, 2, 6]\n",
      "mean concave points is greater than 0.04891999997198582 (has value: 0.1043) --> (Class distribution: [146, 20]\n",
      "AND worst area is greater than 785.7999877929688 (has value: 1575.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n"
     ]
    }
   ],
   "source": [
    "exp_arr = adt.get_explanations(X[:5], y[:5])\n",
    "for exp in exp_arr: \n",
    "    print(\"\\n\")\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa13f9",
   "metadata": {},
   "source": [
    "## Example wtih Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a08f62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boston\n",
      "Standard DT: Training MSE: 11.39, Testing MSE: 29.76, Complexity: 29\n",
      "Additive DT: Training MSE: 9.35, Testing MSE: 27.93, Complexity: 40\n"
     ]
    }
   ],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does not properly test its accuracy\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = mean_squared_error(y_train, y_pred_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = mean_squared_error(y_test, y_pred_test)\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training MSE: {round(score_train,2)}, Testing MSE: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeRegressor(max_depth=4, min_samples_leaf=5, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "    \n",
    "X,y = get_boston()\n",
    "adt = evaluate_dataset(\"Boston\", X,y)\n",
    "\n",
    "# X,y = get_diabetes()\n",
    "# adt = evaluate_dataset(\"Diabetes\", X,y)\n",
    "\n",
    "# X,y = get_linnerud()\n",
    "# adt = evaluate_dataset(\"Linnerud\", X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5066d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************\n",
      "Generated Tree\n",
      "********************************************************\n",
      "# Nodes: 19\n",
      "Left Chidren: [1, 3, 5, 7, -2, 9, 11, -2, -2, 13, -2, 15, 17, -2, -2, -2, -2, -2, -2]\n",
      "Right Chidren: [2, 4, 6, 8, -2, 10, 12, -2, -2, 14, -2, 16, 18, -2, -2, -2, -2, -2, -2]\n",
      "# Rows:  [379, 135, 244, 112, 23, 121, 123, 58, 54, 108, 13, 64, 59, 5, 103, 13, 51, 27, 32]\n",
      "Features: [12, 5, 12, 5, -100, 5, 0, -100, -100, 2, -100, 4, 12, -2, -2, -2, -2, -2, -2]\n",
      "Features in additive nodes: [[], [], [], [], [0, 5, 10, 4, 8, 12], [], [], [6, 5, 0, 7, 12, 2], [4, 6, 7], [], [4, 6, 7, 0, 11, 10], [], [], [], [], [], [], [], []]\n",
      "Thresholds: [8.130000114440918, 7.434999942779541, 15.0, 6.6565001010894775, 5.204999923706055, 6.60450005531311, 5.769209861755371, 16.570000171661377, 91.29999923706055, 2.850000023841858, 16.899999618530273, 0.5309999883174896, 20.3149995803833, -2, -2, -2, -2, -2, -2]\n",
      "Depths: [0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]\n",
      "Can split:  [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Average target value: [22.608707124010557, 31.158518518518516, 17.87827868852459, 28.375892857142862, 44.70869565217391, 21.48925619834711, 14.326016260162604, 24.74137931034483, 32.279629629629625, 20.843518518518515, 26.853846153846153, 16.579687500000002, 11.881355932203391, 26.860000000000003, 20.551456310679608, 19.723076923076924, 15.778431372549019, 13.92962962962963, 10.153125]\n",
      "Average target values in addditive nodes: [[], [], [], [], [(45.74545454545454, 21.9), (45.74545454545454, 21.9), (46.24761904761905, 28.55), (45.74545454545454, 21.9), (45.74545454545454, 21.9), (46.44117647058823, 39.800000000000004)], [], [], [(24.14909090909091, 35.6), (38.75, 24.241071428571427), (24.298245614035082, 50.0), (50.0, 24.298245614035082), (50.0, 24.298245614035082), (24.173584905660377, 30.76)], [(31.43877551020408, 40.519999999999996), (31.304166666666664, 40.083333333333336), (50.0, 31.59807692307692)], [], [(27.841666666666665, 15.0), (27.841666666666665, 15.0), (15.0, 27.841666666666665), (27.841666666666665, 15.0), (27.841666666666665, 15.0), (30.9, 24.325)], [], [], [], [], [], [], [], []]\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9859947",
   "metadata": {},
   "source": [
    "## Example Tuning Hyperparameters with a Cross Validated Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6555065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_score:  26.448151685940985\n",
      "best estimator:  min_samples_split: 5, min_samples_leaf: 5, max_depth: 5, allow_additive_nodes: True, max_added_splits_per_node: 3\n"
     ]
    }
   ],
   "source": [
    "# Note: this can be several minutes to execute.\n",
    "\n",
    "X,y = get_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': (5,10,25,50), \n",
    "    'min_samples_leaf': (5,10,15),\n",
    "    'max_depth': (4,5,6,7),\n",
    "    'allow_additive_nodes': (True, False),\n",
    "    'max_added_splits_per_node': (2,3,4,5,10)\n",
    "}\n",
    "\n",
    "estimator = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5)\n",
    "gs_estimator = RandomizedSearchCV(estimator, parameters, scoring='neg_mean_squared_error',n_iter=100)\n",
    "gs_estimator.fit(X_train, y_train)\n",
    "y_pred = gs_estimator.predict(X_test)\n",
    "test_score = mean_squared_error(list(y_pred), list(y_test)) \n",
    "\n",
    "print(\"test_score: \", test_score)\n",
    "print(\"best estimator: \", gs_estimator.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
