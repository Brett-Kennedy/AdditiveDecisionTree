{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7283a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_diabetes, make_regression \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "\n",
    "# If AdditiveDecisionTree.py is not in the current folder, specify the path \n",
    "import sys  \n",
    "sys.path.insert(0, 'C:\\python_projects\\AdditiveDecisionTree_project\\AdditiveDecisionTree') \n",
    "from AdditiveDecisionTree import AdditiveDecisionTreeClasssifier, AdditiveDecisionTreeRegressor\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74761305",
   "metadata": {},
   "source": [
    "## Methods used to load the toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80859ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification datasets \n",
    "\n",
    "def get_iris():\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X = pd.DataFrame(X, columns=iris['feature_names'])\n",
    "    y = pd.Series(y)\n",
    "    return X, y\n",
    "\n",
    "def get_breast_cancer():\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    return X, y\n",
    "\n",
    "def get_wine():\n",
    "    X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "    return X, y\n",
    "\n",
    "# Regression datasets\n",
    "\n",
    "def get_diabetes():\n",
    "    data = load_diabetes()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    return X, y\n",
    "\n",
    "# def get_linnerud():\n",
    "#     data = load_linnerud(as_frame=True)\n",
    "#     X = data.data\n",
    "#     y = data.target['Weight']\n",
    "#     return X,y\n",
    "\n",
    "def get_make_regression():\n",
    "    np.random.seed(0)\n",
    "    X, y = make_regression(noise=0.0)\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fac78",
   "metadata": {},
   "source": [
    "## Example using sklearn's Decision Tree and AddtiveDecisionTree on toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898c2ca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris\n",
      "Standard DT: Training score: 1.0, Testing score: 0.97, Complexity: 13\n",
      "Additive DT: Training score: 0.96, Testing score: 0.88, Complexity: 5\n",
      "\n",
      "Wine\n",
      "Standard DT: Training score: 1.0, Testing score: 0.92, Complexity: 13\n",
      "Additive DT: Training score: 0.97, Testing score: 0.95, Complexity: 7\n",
      "\n",
      "Breast Cancer\n",
      "Standard DT: Training score: 0.99, Testing score: 0.92, Complexity: 23\n",
      "Additive DT: Training score: 0.97, Testing score: 0.91, Complexity: 11\n"
     ]
    }
   ],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does not \n",
    "# properly test its accuracy. We can, though, see that in terms of test scores,\n",
    "# ADT (Additive Decision Trees) often do about the same as DT (standard Decsion\n",
    "# Trees), but sometimes one or the other does better. \n",
    "# Training scores are also show to give a sense of overfitting.\n",
    "\n",
    "# To estimate complexity for DTs, we use the number of nodes\n",
    "# To estimate complexity for ADTs, we call get_model_complexity(),\n",
    "# which is similar, but considers that additive nodes are more complex.\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training score: {round(score_train,2)}, Testing score: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "    \n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeClasssifier(max_depth=4, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "    \n",
    "    \n",
    "X,y = get_iris()\n",
    "evaluate_dataset(\"Iris\", X,y)\n",
    "\n",
    "X,y = get_wine()\n",
    "evaluate_dataset(\"Wine\", X,y)\n",
    "\n",
    "X,y = get_breast_cancer()\n",
    "adt = evaluate_dataset(\"Breast Cancer\", X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e883f6",
   "metadata": {},
   "source": [
    "## Summary Output of the AdditiveDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575195ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************\n",
      "Generated Tree\n",
      "********************************************************\n",
      "\n",
      "# Nodes: 9\n",
      "\n",
      "Left Chidren:\n",
      "[1, 3, 5, -2, -2, 7, -2, -2, -2]\n",
      "\n",
      "Right Chidren:\n",
      "[2, 4, 6, -2, -2, 8, -2, -2, -2]\n",
      "\n",
      "# Rows: \n",
      "[426, 260, 166, 252, 8, 30, 136, 14, 16]\n",
      "\n",
      "Features:\n",
      "[7, 20, 23, -100, -2, 21, -2, -2, -2]\n",
      "\n",
      "Features in additive nodes:\n",
      "[[], [], [], [1, 13], [], [], [], [], []]\n",
      "\n",
      "Thresholds:\n",
      "[0.04891999997198582, 17.589999198913574, 785.7999877929688, 21.574999809265137, -2, 23.739999771118164, -2, -2, -2]\n",
      "\n",
      "Depths:\n",
      "[0, 1, 1, 2, 2, 2, 2, 3, 3]\n",
      "\n",
      "Can split: \n",
      "[True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Class counts:\n",
      "[[159, 267], [13, 247], [146, 20], [7, 245], [6, 2], [13, 17], [133, 3], [0, 14], [13, 3]]\n",
      "\n",
      "Leaf Class Counts:\n",
      "[[7, 245], [6, 2], [133, 3], [0, 14], [13, 3]]\n",
      "\n",
      "Node igr: \n",
      "[0.4156254639152989, 0.2031712696855239, 0.29661687709662865, 0.18239393289682015, -2, 0.43241893359216155, -2, -2, -2]\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This continues the example with the Breast Cancer dataset.\n",
    "\n",
    "# The output to explain an Additive Decsion Tree is similar as for\n",
    "# scikit-learn decision trees, though has slighly more information.\n",
    "# For example, it provides the depth of each node and the class counts \n",
    "# in each node. \n",
    "\n",
    "# Here node 3 is an additive node. In the features list, it is specified\n",
    "# as feature -100. In the Features in addtivie nodes list, we see it\n",
    "# uses both feature 1 and feature 13. \n",
    "\n",
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9add9",
   "metadata": {},
   "source": [
    "## Explanations of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369d85db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial distribution of classes: [0, 1]: [159, 267]\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 0: 0 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 2, 6]\n",
      "\n",
      "mean concave points is greater than 0.04891999997198582 \n",
      "    (has value: 0.1471) --> (Class distribution: [146, 20]\n",
      "\n",
      "AND worst area is greater than 785.7999877929688 \n",
      "    (has value: 2019.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 1: 0 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 2, 6]\n",
      "\n",
      "mean concave points is greater than 0.04891999997198582 \n",
      "    (has value: 0.07017) --> (Class distribution: [146, 20]\n",
      "\n",
      "AND worst area is greater than 785.7999877929688 \n",
      "    (has value: 1956.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 2: 0 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 2, 6]\n",
      "\n",
      "mean concave points is greater than 0.04891999997198582 \n",
      "    (has value: 0.1279) --> (Class distribution: [146, 20]\n",
      "\n",
      "AND worst area is greater than 785.7999877929688 \n",
      "    (has value: 1709.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 3: 0 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 2, 5, 8]\n",
      "\n",
      "mean concave points is greater than 0.04891999997198582 \n",
      "    (has value: 0.1052) --> (Class distribution: [146, 20]\n",
      "\n",
      "AND worst area is less than 785.7999877929688 \n",
      "    (has value: 567.7) --> (Class distribution: [13, 17]\n",
      "\n",
      "AND worst texture is greater than 23.739999771118164 \n",
      "    (has value: 26.5) --> (Class distribution: [13, 3]\n",
      "where the majority class is: 0\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 4: 0 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 2, 6]\n",
      "\n",
      "mean concave points is greater than 0.04891999997198582 \n",
      "    (has value: 0.1043) --> (Class distribution: [146, 20]\n",
      "\n",
      "AND worst area is greater than 785.7999877929688 \n",
      "    (has value: 1575.0) --> (Class distribution: [133, 3]\n",
      "where the majority class is: 0\n"
     ]
    }
   ],
   "source": [
    "# This provides explanations (in the form of the decision path)\n",
    "# for the first five rows. \n",
    "\n",
    "exp_arr = adt.get_explanations(X[:5], y[:5])\n",
    "for exp in exp_arr: \n",
    "    print(\"\\n\")\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e9e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial distribution of classes: [0, 1]: [159, 267]\n",
      "\n",
      "\n",
      "...............................................................\n",
      "Prediction for row 0: 1 -- Correct\n",
      "...............................................................\n",
      "Path: [0, 1, 3]\n",
      "\n",
      "mean concave points is less than 0.04891999997198582 \n",
      "    (has value: 0.04781) --> (Class distribution: [13, 247]\n",
      "\n",
      "AND worst radius is less than 17.589999198913574 \n",
      "    (has value: 15.11) --> (Class distribution: [7, 245]\n",
      "\n",
      "AND vote based on: \n",
      "  1: mean texture is less than 21.574999809265137\n",
      "     (has value 14.36)  --> (class distribution: [1, 209])\n",
      "  2: area error is less than 42.19000053405762\n",
      "     (has value 23.56)  --> (class distribution: [4, 243])\n",
      "The class with the most votes is 1\n"
     ]
    }
   ],
   "source": [
    "# This gives an example (Row 19) where the decision path includes \n",
    "# node 3, which is an additive node. \n",
    "\n",
    "exp_arr = adt.get_explanations(X.loc[19:19], y.loc[19:19])\n",
    "for exp in exp_arr: \n",
    "    print(\"\\n\")\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa13f9",
   "metadata": {},
   "source": [
    "## Example wtih Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a08f62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diabetes\n",
      "Standard DT: Training MSE: 2281.54, Testing MSE: 4373.97, Complexity: 29\n",
      "Additive DT: Training MSE: 2159.58, Testing MSE: 4291.76, Complexity: 33\n",
      "\n",
      "Make Regression\n",
      "Standard DT: Training MSE: 3487.28, Testing MSE: 23856.35, Complexity: 17\n",
      "Additive DT: Training MSE: 3302.9, Testing MSE: 21077.32, Complexity: 20\n"
     ]
    }
   ],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does \n",
    "# not properly test its accuracy\n",
    "\n",
    "# In these examples, the additive decision trees provide slightly lower errors\n",
    "# but slightly higher complexity.\n",
    "\n",
    "# In general, Additive Decision Trees tend to work better for classification \n",
    "# than regression at least with default hyperparameters.\n",
    "\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = mean_squared_error(y_train, y_pred_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = mean_squared_error(y_test, y_pred_test)\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training MSE: {round(score_train,2)}, Testing MSE: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "    \n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeRegressor(max_depth=4, min_samples_leaf=5, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "  \n",
    "    \n",
    "X,y = get_diabetes()\n",
    "adt = evaluate_dataset(\"Diabetes\", X, y)\n",
    "\n",
    "X,y = get_make_regression()\n",
    "adt = evaluate_dataset(\"Make Regression\", X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5066d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************\n",
      "Generated Tree\n",
      "********************************************************\n",
      "\n",
      "# Nodes: 13\n",
      "\n",
      "Left Chidren:\n",
      "[1, 3, 5, -2, 7, 9, -2, 11, -2, -2, -2, -2, -2]\n",
      "\n",
      "Right Chidren:\n",
      "[2, 4, 6, -2, 8, 10, -2, 12, -2, -2, -2, -2, -2]\n",
      "\n",
      "# Rows: \n",
      "[75, 53, 22, 7, 46, 16, 6, 31, 15, 11, 5, 25, 6]\n",
      "\n",
      "Features:\n",
      "[57, 53, 46, -2, 43, 41, -2, 14, -100, -100, -2, -2, -2]\n",
      "\n",
      "Features in additive nodes:\n",
      "[[], [], [], [], [], [], [], [], [32, 96], [72, 15, 79, 85, 96], [], [], []]\n",
      "\n",
      "Thresholds:\n",
      "[0.2633100152015686, -0.9771790504455566, 0.5912367105484009, -2, 0.3434883654117584, 1.1032692193984985, -2, 0.5415648818016052, -0.8923328518867493, -0.13171404972672462, -2, -2, -2]\n",
      "\n",
      "Depths:\n",
      "[0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4]\n",
      "\n",
      "Can split: \n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Average target values:\n",
      "[-28.94558781986425, -72.30996714891963, 75.5231442001328, 66.46907443748522, -93.42851695554644, 35.158511395759106, 183.16216501179596, -125.21029964481193, -27.746166064397826, -6.244026611424963, 126.24409501156406, -154.15960673535238, -4.588186767560032]\n",
      "\n",
      "Average target values in additive nodes:[[], [], [], [], [], [], [], [], [(-132.02410519598638, 10.173084528907104), (-80.16582925228708, 32.16202043604704)], [(21.68185255974749, -80.7130377345515), (-75.58467168872085, 33.37919914702983), (36.54293366212939, -57.58837893969019), (33.37919914702983, -75.58467168872085), (-68.33470394239224, 29.23636043484205)], [], [], []]\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9859947",
   "metadata": {},
   "source": [
    "## Example Tuning Hyperparameters with a Cross Validated Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6555065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_score:  4277.794998844322\n",
      "best estimator:  min_samples_split: 25, min_samples_leaf: 15, max_depth: 5, allow_additive_nodes: True, max_added_splits_per_node: 5\n"
     ]
    }
   ],
   "source": [
    "# Note: this can be several minutes to execute.\n",
    "\n",
    "X,y = get_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': (5,10,25,50), \n",
    "    'min_samples_leaf': (5,10,15),\n",
    "    'max_depth': (4,5,6,7),\n",
    "    'allow_additive_nodes': (True, False),\n",
    "    'max_added_splits_per_node': (2,3,4,5,10)\n",
    "}\n",
    "\n",
    "estimator = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5)\n",
    "gs_estimator = RandomizedSearchCV(estimator, parameters, scoring='neg_mean_squared_error',n_iter=100)\n",
    "gs_estimator.fit(X_train, y_train)\n",
    "y_pred = gs_estimator.predict(X_test)\n",
    "test_score = mean_squared_error(list(y_pred), list(y_test)) \n",
    "\n",
    "print(\"test_score: \", test_score)\n",
    "print(\"best estimator: \", gs_estimator.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f967745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the best model found during tuning\n",
    "\n",
    "adt = AdditiveDecisionTreeRegressor(\n",
    "        min_samples_split=25, \n",
    "        min_samples_leaf=15, \n",
    "        max_depth=5, \n",
    "        allow_additive_nodes=True, \n",
    "        max_added_splits_per_node=5)\n",
    "adt.fit(X_train, y_train)\n",
    "\n",
    "adt.get_model_complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f204324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
