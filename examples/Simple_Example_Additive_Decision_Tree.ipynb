{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_diabetes, make_regression \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "\n",
    "# If AdditiveDecisionTree.py is not in the current folder, specify the path \n",
    "import sys  \n",
    "sys.path.insert(0, 'C:\\python_projects\\AdditiveDecisionTree_project\\AdditiveDecisionTree') \n",
    "from AdditiveDecisionTree import AdditiveDecisionTreeClasssifier, AdditiveDecisionTreeRegressor\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74761305",
   "metadata": {},
   "source": [
    "## Methods used to load the toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80859ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification datasets \n",
    "\n",
    "def get_iris():\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    X = pd.DataFrame(X, columns=iris['feature_names'])\n",
    "    y = pd.Series(y)\n",
    "    return X, y\n",
    "\n",
    "def get_breast_cancer():\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    return X, y\n",
    "\n",
    "def get_wine():\n",
    "    X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "    return X, y\n",
    "\n",
    "# Regression datasets\n",
    "\n",
    "def get_diabetes():\n",
    "    data = load_diabetes()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    return X, y\n",
    "\n",
    "# def get_linnerud():\n",
    "#     data = load_linnerud(as_frame=True)\n",
    "#     X = data.data\n",
    "#     y = data.target['Weight']\n",
    "#     return X,y\n",
    "\n",
    "def get_make_regression():\n",
    "    np.random.seed(0)\n",
    "    X, y = make_regression(noise=0.0)\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fac78",
   "metadata": {},
   "source": [
    "## Example using sklearn's Decision Tree and AddtiveDecisionTree on toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c2ca6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does not \n",
    "# properly test its accuracy. We can, though, see that in terms of test scores,\n",
    "# ADT (Additive Decision Trees) often do about the same as DT (standard Decsion\n",
    "# Trees), but sometimes one or the other does better. \n",
    "# Training scores are also show to give a sense of overfitting.\n",
    "\n",
    "# To estimate complexity for DTs, we use the number of nodes\n",
    "# To estimate complexity for ADTs, we call get_model_complexity(),\n",
    "# which is similar, but considers that additive nodes are more complex.\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training score: {round(score_train,2)}, Testing score: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "    \n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeClasssifier(max_depth=4, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "    \n",
    "    \n",
    "X,y = get_iris()\n",
    "evaluate_dataset(\"Iris\", X,y)\n",
    "\n",
    "X,y = get_wine()\n",
    "evaluate_dataset(\"Wine\", X,y)\n",
    "\n",
    "X,y = get_breast_cancer()\n",
    "adt = evaluate_dataset(\"Breast Cancer\", X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e883f6",
   "metadata": {},
   "source": [
    "## Summary Output of the AdditiveDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575195ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This continues the example with the Breast Cancer dataset.\n",
    "\n",
    "# The output to explain an Additive Decsion Tree is similar as for\n",
    "# scikit-learn decision trees, though has slighly more information.\n",
    "# For example, it provides the depth of each node and the class counts \n",
    "# in each node. \n",
    "\n",
    "# Here node 3 is an additive node. In the features list, it is specified\n",
    "# as feature -100. In the Features in addtivie nodes list, we see it\n",
    "# uses both feature 1 and feature 13. \n",
    "\n",
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9add9",
   "metadata": {},
   "source": [
    "## Explanations of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d85db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This provides explanations (in the form of the decision path)\n",
    "# for the first five rows. \n",
    "\n",
    "exp_arr = adt.get_explanations(X[:5], y[:5])\n",
    "for exp in exp_arr: \n",
    "    print(\"\\n\")\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929beea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives an example (Row 19) where the decision path includes \n",
    "# node 3, which is an additive node. \n",
    "\n",
    "exp_arr = adt.get_explanations(X.loc[19:19], y.loc[19:19])\n",
    "for exp in exp_arr: \n",
    "    print(\"\\n\")\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa13f9",
   "metadata": {},
   "source": [
    "## Example wtih Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a08f62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: this provides only an example of using AdditiveDecisionTree and does \n",
    "# not properly test its accuracy\n",
    "\n",
    "# In these examples, the additive decision trees provide slightly lower errors\n",
    "# but slightly higher complexity.\n",
    "\n",
    "# In general, Additive Decision Trees tend to work better for classification \n",
    "# than regression at least with default hyperparameters.\n",
    "\n",
    "\n",
    "def evaluate_model(clf, clf_desc, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    score_train = mean_squared_error(y_train, y_pred_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    score_test = mean_squared_error(y_test, y_pred_test)\n",
    "    complexity = 0\n",
    "    if hasattr(clf, \"get_model_complexity\"):\n",
    "        complexity = clf.get_model_complexity()\n",
    "    elif hasattr(clf, \"tree_\"):\n",
    "        complexity = len(clf.tree_.feature)\n",
    "    print(f\"{clf_desc}: Training MSE: {round(score_train,2)}, Testing MSE: {round(score_test,2)}, Complexity: {complexity}\")\n",
    "\n",
    "    \n",
    "def evaluate_dataset(dataset_name, X,y):\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    dt_1 = tree.DecisionTreeRegressor(max_depth=4, min_samples_leaf=5, random_state=42)\n",
    "    evaluate_model(dt_1, \"Standard DT\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    adt = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5, allow_additive_nodes=True, verbose_level=0)\n",
    "    evaluate_model(adt, \"Additive DT\", X_train, X_test, y_train, y_test)\n",
    "    return adt\n",
    "  \n",
    "    \n",
    "X,y = get_diabetes()\n",
    "adt = evaluate_dataset(\"Diabetes\", X, y)\n",
    "\n",
    "X,y = get_make_regression()\n",
    "adt = evaluate_dataset(\"Make Regression\", X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5066d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adt.output_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9859947",
   "metadata": {},
   "source": [
    "## Example Tuning Hyperparameters with a Cross Validated Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6555065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: this can be several minutes to execute.\n",
    "\n",
    "X,y = get_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': (5,10,25,50), \n",
    "    'min_samples_leaf': (5,10,15),\n",
    "    'max_depth': (4,5,6,7),\n",
    "    'allow_additive_nodes': (True, False),\n",
    "    'max_added_splits_per_node': (2,3,4,5,10)\n",
    "}\n",
    "\n",
    "estimator = AdditiveDecisionTreeRegressor(max_depth=4, min_samples_leaf=5)\n",
    "gs_estimator = RandomizedSearchCV(estimator, parameters, scoring='neg_mean_squared_error',n_iter=100)\n",
    "gs_estimator.fit(X_train, y_train)\n",
    "y_pred = gs_estimator.predict(X_test)\n",
    "test_score = mean_squared_error(list(y_pred), list(y_test)) \n",
    "\n",
    "print(\"test_score: \", test_score)\n",
    "print(\"best estimator: \", gs_estimator.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the best model found during tuning\n",
    "\n",
    "adt = AdditiveDecisionTreeRegressor(\n",
    "        min_samples_split=25, \n",
    "        min_samples_leaf=15, \n",
    "        max_depth=5, \n",
    "        allow_additive_nodes=True, \n",
    "        max_added_splits_per_node=5)\n",
    "adt.fit(X_train, y_train)\n",
    "\n",
    "adt.get_model_complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87babcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
